{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pandas, Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns and rows in dataframes\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes For The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IterativeImputer class for imputing missing values\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# combines data from all counties regarding personnel associated with judicial system\n",
    "class CourtPersonnel:\n",
    "    \n",
    "    # reads in excel file containing personnel data\n",
    "    def __init__(self, excel_file):\n",
    "        self.df = pd.read_excel(excel_file)\n",
    "    \n",
    "    # converts the 'Public Defenders' column to binary\n",
    "    def pdef_binary_transform(self):\n",
    "        self.df['Public Defenders'] = self.df['Public Defenders'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # cleans the personnel dataset by converting and renaming columns, thus returning dataframe\n",
    "    def ct_personnel_clean(self):\n",
    "        new_columns = {'County': 'county', 'District Judges/Magistrates': 'dist_judg_mag',\n",
    "                       'Assistant/Deputy Clerks': 'clerks', 'District Attorneys': 'dist_attys',\n",
    "                       'Public Defenders': 'p_def',\n",
    "                       'Law Enforcement Agencies': 'law_enf_ag'}\n",
    "        self.pdef_binary_transform()\n",
    "        self.df = self.df.rename(columns=new_columns)\n",
    "        self.df.iloc[:, -1] = self.df.iloc[:, -1].astype('int64')\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "# combines population for all years into dataframe\n",
    "class CountyPop:\n",
    "    \n",
    "    # reads in excel file for population\n",
    "    def __init__(self, excel_file):\n",
    "        self.file = excel_file\n",
    "        self.df = None\n",
    "    \n",
    "    # removes unnecessary columns and rows \n",
    "    def slice_df(self):\n",
    "        self.df = pd.read_excel(self.file).iloc[3:-2, 3:10]\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    # converts all columns to integer data type\n",
    "    def change_dtypes(self):\n",
    "        for col in list(self.df.columns.values):\n",
    "            self.df[col] = self.df[col].astype('int64')\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    # cleans dataset and renames dataframe\n",
    "    def county_pop_clean(self):\n",
    "        self.slice_df()\n",
    "        self.change_dtypes()\n",
    "        \n",
    "        return self.df.mean(axis=1).astype('int64').to_frame().reset_index(drop=True).rename(columns={0: 'avg_pop'})\n",
    "    \n",
    "\n",
    "# combines all median income data and returns a dataframe with the average\n",
    "class AvgIncome:\n",
    "\n",
    "    # reads in csv file for median income\n",
    "    def __init__(self, csv_file):\n",
    "\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # computes the average of all median incomes and converts column to integer data type\n",
    "    def clean_avg_income(self):\n",
    "\n",
    "        self.df = self.df.mean(axis=1).to_frame().rename(columns={0: 'avg_income'})\n",
    "        self.df['avg_income'] = self.df['avg_income'].astype('int64')\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "# combines violent crime data and returns single column of averages\n",
    "class ViolentCrimeData:\n",
    "    \n",
    "    # reads in a list of excel files\n",
    "    def __init__(self, a_list):\n",
    "        self.file_list = a_list\n",
    "        self.file_dict = {}\n",
    "        self.years = None\n",
    "        self.files = None\n",
    "        self.df = None\n",
    "        self.concat_df = None\n",
    "    \n",
    "    # creates dictionary of files from init method with years as keys and file names as values\n",
    "    def create_file_dict(self):\n",
    "        for file in self.file_list:\n",
    "            self.file_dict[int(file[:4])] = file\n",
    "\n",
    "        return self.file_dict\n",
    "    \n",
    "    # concatenates all files in dictionary and returns dataframe \n",
    "    def concat_h(self):\n",
    "        self.years = list(self.file_dict.keys())\n",
    "        self.files = list(self.file_dict.values())\n",
    "        self.df = pd.read_excel(self.files[0])\n",
    "        self.df = self.split_df()\n",
    "        for i in range(1, len(self.files)):\n",
    "            self.concat_df = pd.read_excel(self.files[i])\n",
    "            self.concat_df = self.split_concat_df()\n",
    "            self.df = pd.concat([self.df, self.concat_df], axis=1)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # slices the column with violent crime rate for self.df\n",
    "    def split_df(self):\n",
    "        return self.df.iloc[:, -1].to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # slices the column with violent crime rate for self.concat_df\n",
    "    def split_concat_df(self):\n",
    "        return self.concat_df.iloc[:, -1].to_frame().reset_index(drop=True)\n",
    "    \n",
    "    # imputes any missing values using the Iterative Imputer class\n",
    "    def iter_imputer(self):\n",
    "        self.df = self.df.replace(to_replace='unknown', value=np.nan)\n",
    "        imp = IterativeImputer()\n",
    "        imp.fit(self.df)\n",
    "        imp_array = imp.transform(self.df)\n",
    "        self.df = pd.DataFrame(data=imp_array)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # uses methods to clean all files, concatenate, fill null values, and rename dataframe\n",
    "    def vcr_load_clean(self):\n",
    "        self.create_file_dict()\n",
    "        self.concat_h()\n",
    "        self.iter_imputer()\n",
    "\n",
    "        self.df = self.df.mean(axis=1).to_frame().reset_index(drop=True)\n",
    "\n",
    "        return self.df.rename(columns={0: 'vcr_per_100000'}).round(1)\n",
    "    \n",
    "\n",
    "# combines divorce data and returns average\n",
    "# inherits ViolentCrimeData class due to synonymous calculations\n",
    "class AvgDivorces(ViolentCrimeData):\n",
    "    \n",
    "    # initializes using list of excel files\n",
    "    def __init__(self, a_list):\n",
    "        super().__init__(a_list)\n",
    "    \n",
    "    # concatenates all divorce datasets and returns averages in a dataframe\n",
    "    def avgdiv_load_clean(self):\n",
    "        self.create_file_dict()\n",
    "        self.concat_h()\n",
    "        self.df = self.df.mean(axis=1).to_frame().reset_index(drop=True)\n",
    "\n",
    "        return self.df.rename(columns={0: 'avg_num_divrs'}).round(1)\n",
    "\n",
    "#combines all opioid overdose deaths and returns dataframe\n",
    "class OpioidDeaths:\n",
    "\n",
    "    # reads in csv file containing deaths by overdose in opioids\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "    # returns one-column dataframe with combined deaths by county and drops all columns with 'NaN' values\n",
    "    def load_and_clean_data(self):\n",
    "        self.df = self.df[['Year', 'County', 'Drug', 'Count']].dropna().sort_values(by=['Year', 'County'],\n",
    "                                                                                    ascending=True)\n",
    "        self.df = self.df[(self.df['Drug'] == 'Any Opioid') & (self.df['Year'] >= 2012)].groupby(['County'])[\n",
    "            'Count'].sum().to_frame().reset_index().rename(columns={'Count':'op_od_deaths'})\n",
    "\n",
    "        return self.df.iloc[:,-1].to_frame().reset_index(drop=True)\n",
    "    \n",
    "# converts case inventory dataset into a column of computed ratios\n",
    "class CaseDisposition:\n",
    "    \n",
    "    # reads in csv file for case inventory\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file, delimiter=';')\n",
    "    \n",
    "    # extracts columns with felony superior court cases\n",
    "    def fel_sup_group(self):\n",
    "        fel_bool = self.df['Case_Type'] == 'Felonies'\n",
    "        sup_bool = self.df['File_Type'] == 'CRS'\n",
    "        self.df = self.df[fel_bool & sup_bool].reset_index()\n",
    "        self.df = self.df.groupby(['County', 'Case Status'])['Number_of_Cases'].sum().reset_index()\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # computes ratio of disposed cases to pending cases\n",
    "    def sep_cases(self):\n",
    "        disp_cases = self.df[self.df['Case Status'] == 'Disposed'].iloc[:, -1].astype(float).to_frame().reset_index(\n",
    "            drop=True)\n",
    "        pending = self.df[self.df['Case Status'] == 'End Pending'].iloc[:, -1].astype(float).to_frame().reset_index(\n",
    "            drop=True)\n",
    "        return disp_cases / pending\n",
    "    \n",
    "    # returns dataframe with case disposition ratio rounded to 3 decimal places and renamed column\n",
    "    def load_clean_cases(self):\n",
    "        self.df = self.fel_sup_group()\n",
    "\n",
    "        return self.sep_cases().rename(columns={'Number_of_Cases': 'case_disp_ratio'}).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets \n",
    "\n",
    "We will use all of these classes for building the preprocessed data set for the model using another class called *CombineAllData*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines all features for case disposition inefficiency model\n",
    "class CombineAllData:\n",
    "\n",
    "    def __init__(self):\n",
    "        # assign names to all needed files\n",
    "        self.ct_personnel_file = 'nc_court_data_by_county.xlsx'\n",
    "        self.population_file = 'county_pop.xlsx'\n",
    "        self.income_file = 'nc_median_income_by_county.csv'\n",
    "        self.divrs_files = ['2012 Divorce Rate By County.xlsx', '2013 Divorce Rate By County.xlsx',\n",
    "                            '2014 Divorce Rate By County.xlsx', '2015 Divorce Rate By County.xlsx',\n",
    "                            '2016 Divorce Rate By County.xlsx', '2017 Divorce Rate By County.xlsx',\n",
    "                            '2018 Divorce Rate By County.xlsx']\n",
    "        self.vcr_files = ['2012 Violent Crimes By County.xlsx', '2013 Violent Crimes By County.xlsx',\n",
    "                          '2014 Violent Crimes By County.xlsx', '2015 Violent Crimes By County.xlsx',\n",
    "                          '2016 Violent Crimes By County.xlsx', '2017 Violent Crimes By County.xlsx',\n",
    "                          '2018 Violent Crimes By County.xlsx']\n",
    "        self.opioid_deaths_file = 'od_deaths.csv'\n",
    "        self.case_disp_file = 'caseload_inventory.csv'\n",
    "        self.preprocessing_df = None\n",
    "    \n",
    "    # creates dataframes from loaded files using classes\n",
    "    def load_all_data(self):\n",
    "        self.personnel_df = CourtPersonnel(self.ct_personnel_file).ct_personnel_clean()\n",
    "        self.population_df = CountyPop(self.population_file).county_pop_clean()\n",
    "        self.income_df = AvgIncome(self.income_file).clean_avg_income()\n",
    "        self.avg_divr_df = AvgDivorces(self.divrs_files).avgdiv_load_clean()\n",
    "        self.vcr_df = ViolentCrimeData(self.vcr_files).vcr_load_clean()\n",
    "        self.opioid_df = OpioidDeaths(self.opioid_deaths_file).load_and_clean_data()\n",
    "        self.case_disp_df = CaseDisposition(self.case_disp_file).load_clean_cases()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # saves preprocessed data in a csv file to be used for model\n",
    "    def create_preprocess_save(self):\n",
    "        concat_list = [self.personnel_df, self.population_df, self.income_df,\n",
    "                       self.avg_divr_df, self.vcr_df, self.opioid_df, self.case_disp_df]\n",
    "        self.preprocessing_df = pd.concat(concat_list, axis=1)\n",
    "\n",
    "        return self.preprocessing_df.to_csv('courts_preprocessing_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Machine Learning Model\n",
    "\n",
    "Now that I have the CombineAllData class, I can use it to create the dataframe that includes all of the features for the preprocessing stage of creating the machine learning model and will highlight the important features.  Lastly, the model will determine whether a county may be in need of further resources to boost case disposition ratio for improved efficiency.  Building the classes in the previous cells allows for me to generate the preprocessing csv file in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = CombineAllData()\n",
    "combine.load_all_data().create_preprocess_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first few rows of the data to view the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>dist_judg_mag</th>\n",
       "      <th>clerks</th>\n",
       "      <th>dist_attys</th>\n",
       "      <th>p_def</th>\n",
       "      <th>law_enf_ag</th>\n",
       "      <th>avg_pop</th>\n",
       "      <th>avg_income</th>\n",
       "      <th>avg_num_divrs</th>\n",
       "      <th>vcr_per_100000</th>\n",
       "      <th>op_od_deaths</th>\n",
       "      <th>case_disp_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alamance</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>158137</td>\n",
       "      <td>51303</td>\n",
       "      <td>510.7</td>\n",
       "      <td>422.2</td>\n",
       "      <td>773</td>\n",
       "      <td>1.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37737</td>\n",
       "      <td>49864</td>\n",
       "      <td>88.4</td>\n",
       "      <td>176.8</td>\n",
       "      <td>225</td>\n",
       "      <td>1.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alleghany</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11224</td>\n",
       "      <td>40420</td>\n",
       "      <td>48.3</td>\n",
       "      <td>185.8</td>\n",
       "      <td>53</td>\n",
       "      <td>1.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anson</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>25949</td>\n",
       "      <td>39278</td>\n",
       "      <td>68.7</td>\n",
       "      <td>511.0</td>\n",
       "      <td>87</td>\n",
       "      <td>1.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashe</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>27212</td>\n",
       "      <td>46667</td>\n",
       "      <td>93.9</td>\n",
       "      <td>108.2</td>\n",
       "      <td>135</td>\n",
       "      <td>1.377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      county  dist_judg_mag  clerks  dist_attys  p_def  law_enf_ag  avg_pop  \\\n",
       "0   Alamance             15      34           8      0           7   158137   \n",
       "1  Alexander              4      10           0      0           2    37737   \n",
       "2  Alleghany              3       6           2      0           3    11224   \n",
       "3      Anson              4       8           2      0           4    25949   \n",
       "4       Ashe              2       7           0      0           6    27212   \n",
       "\n",
       "   avg_income  avg_num_divrs  vcr_per_100000  op_od_deaths  case_disp_ratio  \n",
       "0       51303          510.7           422.2           773            1.951  \n",
       "1       49864           88.4           176.8           225            1.436  \n",
       "2       40420           48.3           185.8            53            1.744  \n",
       "3       39278           68.7           511.0            87            1.950  \n",
       "4       46667           93.9           108.2           135            1.377  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_df = pd.read_csv('courts_preprocessing_df.csv')\n",
    "preprocessing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model Using a Class\n",
    "\n",
    "The overall goal is to detect any counties in North Carolina likely in need of additional resources to improve case disposition efficiency such as possible increased funding for hiring more law enforcement or creating district court judge and magistrate seats. This class will perform the following functions:\n",
    "\n",
    "1. Read in the preprocessed data as a csv file.\n",
    "2. Sets up the unscaled features and target data.\n",
    "3. Creates dataframe for selected scaled features using the CustomScaler class.\n",
    "4. Runs a logistic regression model based on the scaled data.\n",
    "5. Creates a summary table displaying the coefficients for all features.\n",
    "6. Saves the model (when applicable).\n",
    "7. Connects to MySQL and creates database with predictions and summary table along with storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Scaler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "\n",
    "    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n",
    "        # scaler is nothing but a StandardScaler object\n",
    "        self.scaler = StandardScaler(copy, with_mean, with_std)\n",
    "        # with some columns\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "\n",
    "    # the fit method, which, again based on StandardScaler\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "\n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "\n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n",
    "\n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class for Case Disposition Inefficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pymysql\n",
    "\n",
    "# model class that reads in a csv file providing features and targets\n",
    "class PredictCaseDispIneff:\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.model_file = None\n",
    "        self.scaler_file = None\n",
    "        self.df_preprocessed = None\n",
    "\n",
    "    # separates unscaled inputs and targets into dataframe and array, respectively\n",
    "    def load_and_add_targets(self):\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        self.df_preprocessed = df.copy()\n",
    "        # use median to separate targets for 50/50 split of data\n",
    "        self.inefficiency_cutoff = df['case_disp_ratio'].median()\n",
    "        self.targets = np.where(df['case_disp_ratio'] < self.inefficiency_cutoff, 1, 0)\n",
    "        self.unscaled_inputs = df.iloc[:, 1:-1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # creates inputs for scaling and returns dataframe\n",
    "    def create_scaled_inputs(self):\n",
    "        df_columns = list(self.unscaled_inputs.columns.values)\n",
    "        columns_to_omit = ['p_def']\n",
    "        columns_to_scale = [col for col in df_columns if col not in columns_to_omit]\n",
    "        # use CustomScaler class to only scale selected columns\n",
    "        self.scaled_inputs = CustomScaler(columns_to_scale).fit(self.unscaled_inputs).transform(self.unscaled_inputs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # initializes training model for scaled data using logistic regression\n",
    "    def init_train_model(self):\n",
    "\n",
    "        # create instance of model\n",
    "        self.log_model = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                                            intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
    "                                            multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                                            random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                                            warm_start=False)\n",
    "\n",
    "        # split up training and testing data using train_test_split with random state of best accuracy\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.scaled_inputs,\n",
    "                                                                                self.targets, train_size=0.75,\n",
    "                                                                                random_state=341)\n",
    "\n",
    "        self.log_model.fit(self.X_train, self.y_train)\n",
    "        # gives user ability to call training accuracy\n",
    "        self.log_model_train_score = self.log_model.score(self.X_train, self.y_train)\n",
    "        # gives user ability to call test accuracy\n",
    "        self.log_model_test_score = self.log_model.score(self.X_test, self.y_test)\n",
    "        # assigns model intercept\n",
    "        self.log_model_intercept = self.log_model.intercept_\n",
    "        # assigns model coefficients\n",
    "        self.log_model_coef = self.log_model.coef_\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def create_summary_table(self):\n",
    "\n",
    "        feature_name = self.unscaled_inputs.columns.values\n",
    "        self.summary_table = pd.DataFrame(columns=['feature_name'], data=feature_name)\n",
    "        self.summary_table['coefficient'] = np.transpose(self.log_model_coef)\n",
    "        self.summary_table['odds_ratio'] = np.exp(self.summary_table.coefficient)\n",
    "        self.summary_table = self.summary_table.sort_values('odds_ratio', ascending=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def st_into_mysql(self):\n",
    "        # creates variable for insert query to be applied to \"predicted_outputs\" database in MySQL\n",
    "        self.st_insert_query = 'INSERT INTO summary_table VALUES '\n",
    "\n",
    "        # initiates loops as the number of rows in self.summary_table\n",
    "        for i in range(self.summary_table.shape[0]):\n",
    "            self.st_insert_query += '('\n",
    "            # loops over each columns per row\n",
    "            for j in range(self.summary_table.shape[1]):\n",
    "                # wraps name of feature in single quotation marks for MySQL\n",
    "                if j == 0:\n",
    "\n",
    "                    self.st_insert_query += '\\'' + str(self.summary_table[self.summary_table.columns.values[j]][i]) + \\\n",
    "                                         '\\'' + ','\n",
    "                else:\n",
    "                    self.st_insert_query += str(self.summary_table[self.summary_table.columns.values[j]][i]) + ','\n",
    "            # ends each row with closing parentheses\n",
    "            self.st_insert_query = self.st_insert_query[:-1] + '), '\n",
    "        #\n",
    "        self.st_insert_query = self.st_insert_query[:-2] + ';'\n",
    "\n",
    "        # set up **kwargs to create MySQL database connection\n",
    "        sql_creds = {'database':'predictions', 'user':'root', 'password':'pass_word'}\n",
    "        # establish connection using pymysql module\n",
    "        conn = pymysql.connect(**sql_creds)\n",
    "        # creates cursor for access to the database\n",
    "        cursor = conn.cursor()\n",
    "        # executes insert query for summary table\n",
    "        cursor.execute(self.st_insert_query)\n",
    "        # saves database\n",
    "        conn.commit()\n",
    "        # closes connection\n",
    "        conn.close()\n",
    "        return self\n",
    "\n",
    "    # runs model and returns all functions and their attributes\n",
    "    def run_model(self):\n",
    "\n",
    "        self.load_and_add_targets().create_scaled_inputs().init_train_model().create_summary_table()\n",
    "\n",
    "        return self\n",
    "\n",
    "    # saves model and scaler (if applicable) using pickle module\n",
    "    def save_model(self):\n",
    "        with open('jud_model', 'wb') as file:\n",
    "            pickle.dump(self.model_file, file)\n",
    "        with open('jud_scaler', 'wb') as file:\n",
    "            pickle.dump(self.scaler_file, file)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # same as run model function but creates dataframe with all features with probability and prediction as two\n",
    "    # additional columns from model test results\n",
    "    def run_and_predict_results(self):\n",
    "\n",
    "        self.run_model()\n",
    "        self.df_preprocessed['probability'] = self.log_model.predict_proba(self.scaled_inputs)[:, 1]\n",
    "        self.df_preprocessed['prediction'] = self.log_model.predict(self.scaled_inputs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # saves resultant dataframe from model in MySQL database for storage and management\n",
    "    def insert_into_mysql(self):\n",
    "        # creates variable for insert query to be applied to \"predicted_outputs\" database in MySQL\n",
    "        self.insert_query = 'INSERT INTO predicted_outputs VALUES '\n",
    "\n",
    "        # initiates loops as the number of rows in self.df_preprocessed\n",
    "        for i in range(self.df_preprocessed.shape[0]):\n",
    "            self.insert_query += '('\n",
    "            # loops over each columns per row\n",
    "            for j in range(self.df_preprocessed.shape[1]):\n",
    "                # wraps name of county in single quotation marks for MySQL\n",
    "                if j == 0:\n",
    "\n",
    "                    self.insert_query += '\\'' + str(self.df_preprocessed[self.df_preprocessed.columns.values[j]][i]) + \\\n",
    "                                         '\\'' + ','\n",
    "                else:\n",
    "                    self.insert_query += str(self.df_preprocessed[self.df_preprocessed.columns.values[j]][i]) + ','\n",
    "            # ends each row with closing parentheses\n",
    "            self.insert_query = self.insert_query[:-1] + '), '\n",
    "        # ends query with semi-colon\n",
    "        self.insert_query = self.insert_query[:-2] + ';'\n",
    "\n",
    "        # set up **kwargs to create MySQL database connection\n",
    "        sql_creds = {'database':'predictions', 'user':'root', 'password':'pass_word'}\n",
    "        # establish connection using pymysql module\n",
    "        conn = pymysql.connect(**sql_creds)\n",
    "        # creates cursor for access to the database\n",
    "        cursor = conn.cursor()\n",
    "        # executes insert query for predictions\n",
    "        cursor.execute(self.insert_query)\n",
    "        # saves database\n",
    "        conn.commit()\n",
    "        # closes connection\n",
    "        conn.close()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialze the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessed data into model class\n",
    "county_predict = PredictCaseDispIneff('courts_preprocessing_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target data is determined by all case disposition ratios less than the median.  This approach allows 50% of the data to be represented on each side of the middle value along with maintaining an implicitly balanced train/test dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 1.501\n",
      "\n",
      "   dist_judg_mag  clerks  dist_attys  p_def  law_enf_ag  avg_pop  avg_income  \\\n",
      "0             15      34           8      0           7   158137       51303   \n",
      "1              4      10           0      0           2    37737       49864   \n",
      "2              3       6           2      0           3    11224       40420   \n",
      "3              4       8           2      0           4    25949       39278   \n",
      "4              2       7           0      0           6    27212       46667   \n",
      "\n",
      "   avg_num_divrs  vcr_per_100000  op_od_deaths  \n",
      "0          510.7           422.2           773  \n",
      "1           88.4           176.8           225  \n",
      "2           48.3           185.8            53  \n",
      "3           68.7           511.0            87  \n",
      "4           93.9           108.2           135  \n",
      "\n",
      "[0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
      " 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
      " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# creates unscaled inputs as dataframe and targets as numpy array\n",
    "county_predict.load_and_add_targets()\n",
    "# median value for target data\n",
    "print(\"median:\", round(county_predict.load_and_add_targets().inefficiency_cutoff, 3))\n",
    "print()\n",
    "print(county_predict.load_and_add_targets().unscaled_inputs.head())\n",
    "print()\n",
    "print(county_predict.load_and_add_targets().targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_judg_mag</th>\n",
       "      <th>clerks</th>\n",
       "      <th>dist_attys</th>\n",
       "      <th>p_def</th>\n",
       "      <th>law_enf_ag</th>\n",
       "      <th>avg_pop</th>\n",
       "      <th>avg_income</th>\n",
       "      <th>avg_num_divrs</th>\n",
       "      <th>vcr_per_100000</th>\n",
       "      <th>op_od_deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.782605</td>\n",
       "      <td>0.414820</td>\n",
       "      <td>0.265989</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>0.368872</td>\n",
       "      <td>-0.013650</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.877084</td>\n",
       "      <td>0.365730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.556222</td>\n",
       "      <td>-0.507857</td>\n",
       "      <td>-0.659191</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.964239</td>\n",
       "      <td>-0.402176</td>\n",
       "      <td>-0.175133</td>\n",
       "      <td>-0.445235</td>\n",
       "      <td>-0.665409</td>\n",
       "      <td>-0.437234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.677933</td>\n",
       "      <td>-0.661636</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.764190</td>\n",
       "      <td>-0.571966</td>\n",
       "      <td>-1.234926</td>\n",
       "      <td>-0.518700</td>\n",
       "      <td>-0.608838</td>\n",
       "      <td>-0.689260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.556222</td>\n",
       "      <td>-0.584747</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.564140</td>\n",
       "      <td>-0.477667</td>\n",
       "      <td>-1.363080</td>\n",
       "      <td>-0.481326</td>\n",
       "      <td>1.435247</td>\n",
       "      <td>-0.639441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.799645</td>\n",
       "      <td>-0.623192</td>\n",
       "      <td>-0.659191</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.164041</td>\n",
       "      <td>-0.469578</td>\n",
       "      <td>-0.533896</td>\n",
       "      <td>-0.435159</td>\n",
       "      <td>-1.096603</td>\n",
       "      <td>-0.569108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.677933</td>\n",
       "      <td>-0.700081</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>-0.529334</td>\n",
       "      <td>-0.467575</td>\n",
       "      <td>-0.521082</td>\n",
       "      <td>-0.979062</td>\n",
       "      <td>-0.633580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.312799</td>\n",
       "      <td>-0.277188</td>\n",
       "      <td>0.150342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436108</td>\n",
       "      <td>-0.339141</td>\n",
       "      <td>-0.125420</td>\n",
       "      <td>-0.272106</td>\n",
       "      <td>0.154237</td>\n",
       "      <td>-0.334666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.799645</td>\n",
       "      <td>-0.623192</td>\n",
       "      <td>-0.659191</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.364090</td>\n",
       "      <td>-0.514682</td>\n",
       "      <td>-1.234028</td>\n",
       "      <td>-0.525296</td>\n",
       "      <td>-0.772264</td>\n",
       "      <td>-0.683399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.312799</td>\n",
       "      <td>-0.430967</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036009</td>\n",
       "      <td>-0.420773</td>\n",
       "      <td>-1.046735</td>\n",
       "      <td>-0.459342</td>\n",
       "      <td>-0.361185</td>\n",
       "      <td>-0.498775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.312799</td>\n",
       "      <td>0.299486</td>\n",
       "      <td>0.497284</td>\n",
       "      <td>0</td>\n",
       "      <td>1.436356</td>\n",
       "      <td>0.148060</td>\n",
       "      <td>0.763015</td>\n",
       "      <td>-0.062885</td>\n",
       "      <td>-0.791750</td>\n",
       "      <td>0.285140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.512874</td>\n",
       "      <td>1.375942</td>\n",
       "      <td>0.728579</td>\n",
       "      <td>1</td>\n",
       "      <td>1.636406</td>\n",
       "      <td>0.979498</td>\n",
       "      <td>0.422880</td>\n",
       "      <td>1.314636</td>\n",
       "      <td>-0.107245</td>\n",
       "      <td>1.647834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.312799</td>\n",
       "      <td>0.030371</td>\n",
       "      <td>-0.196601</td>\n",
       "      <td>0</td>\n",
       "      <td>0.436108</td>\n",
       "      <td>-0.067628</td>\n",
       "      <td>-0.175133</td>\n",
       "      <td>-0.048412</td>\n",
       "      <td>-0.653466</td>\n",
       "      <td>0.248509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.782605</td>\n",
       "      <td>0.876159</td>\n",
       "      <td>0.265989</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.764190</td>\n",
       "      <td>0.608383</td>\n",
       "      <td>1.510504</td>\n",
       "      <td>0.365449</td>\n",
       "      <td>-0.996661</td>\n",
       "      <td>1.143784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.191087</td>\n",
       "      <td>-0.046518</td>\n",
       "      <td>-0.312248</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.364090</td>\n",
       "      <td>-0.112930</td>\n",
       "      <td>-0.175133</td>\n",
       "      <td>-0.141847</td>\n",
       "      <td>-0.752779</td>\n",
       "      <td>-0.012308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.677933</td>\n",
       "      <td>-0.700081</td>\n",
       "      <td>-0.659191</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.964239</td>\n",
       "      <td>-0.578345</td>\n",
       "      <td>2.265847</td>\n",
       "      <td>-0.560654</td>\n",
       "      <td>-1.412141</td>\n",
       "      <td>-0.725891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.434510</td>\n",
       "      <td>-0.238743</td>\n",
       "      <td>-0.196601</td>\n",
       "      <td>1</td>\n",
       "      <td>1.036257</td>\n",
       "      <td>-0.197650</td>\n",
       "      <td>0.789723</td>\n",
       "      <td>-0.184534</td>\n",
       "      <td>-0.180159</td>\n",
       "      <td>0.033115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.677933</td>\n",
       "      <td>-0.584747</td>\n",
       "      <td>-0.543543</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.964239</td>\n",
       "      <td>-0.493049</td>\n",
       "      <td>-0.551627</td>\n",
       "      <td>-0.532258</td>\n",
       "      <td>-0.433469</td>\n",
       "      <td>-0.681933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.660894</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.034694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636158</td>\n",
       "      <td>0.358958</td>\n",
       "      <td>-0.175133</td>\n",
       "      <td>0.315617</td>\n",
       "      <td>-0.024904</td>\n",
       "      <td>0.636803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.556222</td>\n",
       "      <td>-0.469412</td>\n",
       "      <td>-0.312248</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.764190</td>\n",
       "      <td>-0.199724</td>\n",
       "      <td>1.858494</td>\n",
       "      <td>-0.331098</td>\n",
       "      <td>-0.603810</td>\n",
       "      <td>-0.545664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.434510</td>\n",
       "      <td>-0.546302</td>\n",
       "      <td>-0.427896</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.364090</td>\n",
       "      <td>-0.463802</td>\n",
       "      <td>-0.829592</td>\n",
       "      <td>-0.416655</td>\n",
       "      <td>-0.302100</td>\n",
       "      <td>-0.538338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dist_judg_mag    clerks  dist_attys  p_def  law_enf_ag   avg_pop  \\\n",
       "0        0.782605  0.414820    0.265989      0    0.036009  0.368872   \n",
       "1       -0.556222 -0.507857   -0.659191      0   -0.964239 -0.402176   \n",
       "2       -0.677933 -0.661636   -0.427896      0   -0.764190 -0.571966   \n",
       "3       -0.556222 -0.584747   -0.427896      0   -0.564140 -0.477667   \n",
       "4       -0.799645 -0.623192   -0.659191      0   -0.164041 -0.469578   \n",
       "5       -0.677933 -0.700081   -0.427896      0    0.036009 -0.529334   \n",
       "6       -0.312799 -0.277188    0.150342      1    0.436108 -0.339141   \n",
       "7       -0.799645 -0.623192   -0.659191      0   -0.364090 -0.514682   \n",
       "8       -0.312799 -0.430967   -0.427896      0    0.036009 -0.420773   \n",
       "9       -0.312799  0.299486    0.497284      0    1.436356  0.148060   \n",
       "10       1.512874  1.375942    0.728579      1    1.636406  0.979498   \n",
       "11      -0.312799  0.030371   -0.196601      0    0.436108 -0.067628   \n",
       "12       0.782605  0.876159    0.265989      0   -0.764190  0.608383   \n",
       "13      -0.191087 -0.046518   -0.312248      0   -0.364090 -0.112930   \n",
       "14      -0.677933 -0.700081   -0.659191      1   -0.964239 -0.578345   \n",
       "15      -0.434510 -0.238743   -0.196601      1    1.036257 -0.197650   \n",
       "16      -0.677933 -0.584747   -0.543543      0   -0.964239 -0.493049   \n",
       "17       0.660894  0.568600    0.034694      0    0.636158  0.358958   \n",
       "18      -0.556222 -0.469412   -0.312248      1   -0.764190 -0.199724   \n",
       "19      -0.434510 -0.546302   -0.427896      0   -0.364090 -0.463802   \n",
       "\n",
       "    avg_income  avg_num_divrs  vcr_per_100000  op_od_deaths  \n",
       "0    -0.013650       0.328442        0.877084      0.365730  \n",
       "1    -0.175133      -0.445235       -0.665409     -0.437234  \n",
       "2    -1.234926      -0.518700       -0.608838     -0.689260  \n",
       "3    -1.363080      -0.481326        1.435247     -0.639441  \n",
       "4    -0.533896      -0.435159       -1.096603     -0.569108  \n",
       "5    -0.467575      -0.521082       -0.979062     -0.633580  \n",
       "6    -0.125420      -0.272106        0.154237     -0.334666  \n",
       "7    -1.234028      -0.525296       -0.772264     -0.683399  \n",
       "8    -1.046735      -0.459342       -0.361185     -0.498775  \n",
       "9     0.763015      -0.062885       -0.791750      0.285140  \n",
       "10    0.422880       1.314636       -0.107245      1.647834  \n",
       "11   -0.175133      -0.048412       -0.653466      0.248509  \n",
       "12    1.510504       0.365449       -0.996661      1.143784  \n",
       "13   -0.175133      -0.141847       -0.752779     -0.012308  \n",
       "14    2.265847      -0.560654       -1.412141     -0.725891  \n",
       "15    0.789723      -0.184534       -0.180159      0.033115  \n",
       "16   -0.551627      -0.532258       -0.433469     -0.681933  \n",
       "17   -0.175133       0.315617       -0.024904      0.636803  \n",
       "18    1.858494      -0.331098       -0.603810     -0.545664  \n",
       "19   -0.829592      -0.416655       -0.302100     -0.538338  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates scaled inputs dataframe\n",
    "county_predict.create_scaled_inputs()\n",
    "# view scaled inputs at a glance\n",
    "county_predict.create_scaled_inputs().scaled_inputs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# runs model \n",
    "county_predict.run_and_predict_results()\n",
    "print('Train Score:', county_predict.run_and_predict_results().log_model_train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p_def</td>\n",
       "      <td>0.566896</td>\n",
       "      <td>1.762788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dist_judg_mag</td>\n",
       "      <td>0.465631</td>\n",
       "      <td>1.593020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>op_od_deaths</td>\n",
       "      <td>0.345726</td>\n",
       "      <td>1.413015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>law_enf_ag</td>\n",
       "      <td>0.334558</td>\n",
       "      <td>1.397323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dist_attys</td>\n",
       "      <td>-0.139540</td>\n",
       "      <td>0.869758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vcr_per_100000</td>\n",
       "      <td>-0.222799</td>\n",
       "      <td>0.800275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avg_income</td>\n",
       "      <td>-0.282070</td>\n",
       "      <td>0.754221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_pop</td>\n",
       "      <td>-0.481183</td>\n",
       "      <td>0.618052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clerks</td>\n",
       "      <td>-0.637300</td>\n",
       "      <td>0.528718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avg_num_divrs</td>\n",
       "      <td>-0.671237</td>\n",
       "      <td>0.511076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_name  coefficient  odds_ratio\n",
       "3           p_def     0.566896    1.762788\n",
       "0   dist_judg_mag     0.465631    1.593020\n",
       "9    op_od_deaths     0.345726    1.413015\n",
       "4      law_enf_ag     0.334558    1.397323\n",
       "2      dist_attys    -0.139540    0.869758\n",
       "8  vcr_per_100000    -0.222799    0.800275\n",
       "6      avg_income    -0.282070    0.754221\n",
       "5         avg_pop    -0.481183    0.618052\n",
       "1          clerks    -0.637300    0.528718\n",
       "7   avg_num_divrs    -0.671237    0.511076"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view summary table with coefficients and odds ratios\n",
    "\n",
    "county_predict.run_and_predict_results().summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the model, public defenders, number of district court judges and magistrates, number of opioid overdose deaths, and number of law enforcement agencies are important features affecting case disposition inefficiency for each county since each of the odd ratios is greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          county  dist_judg_mag  clerks  dist_attys  p_def  law_enf_ag  \\\n",
      "80    Rutherford              8      20           4      1           6   \n",
      "81       Sampson              5      20           7      0           5   \n",
      "82      Scotland              4      14           3      1           3   \n",
      "83        Stanly              6      16           5      0           9   \n",
      "84        Stokes              4      12           2      0           5   \n",
      "85         Surry              6      20           6      0           6   \n",
      "86         Swain              3       5           0      0           3   \n",
      "87  Transylvania              4       9           2      1           3   \n",
      "88       Tyrrell              3       5           1      1           1   \n",
      "89         Union              9      41          12      0           8   \n",
      "90         Vance              9      13           2      0           4   \n",
      "91          Wake             42     132          37      1          32   \n",
      "92        Warren              4       7           1      0           4   \n",
      "93    Washington              1       6           0      1           4   \n",
      "94       Watauga              4      15           3      0           7   \n",
      "95         Wayne             12      32           9      1          11   \n",
      "96        Wilkes              9      20           5      0           3   \n",
      "97        Wilson              8      23           4      0          11   \n",
      "98        Yadkin              4      10           1      0           7   \n",
      "99        Yancey              5       5           3      0           4   \n",
      "\n",
      "    avg_pop  avg_income  avg_num_divrs  vcr_per_100000  op_od_deaths  \\\n",
      "80    67882       41725          241.6           204.0           411   \n",
      "81    63971       44244          182.0           239.6           255   \n",
      "82    35922       37506           82.9           698.5           163   \n",
      "83    61563       55243          189.6           244.5           415   \n",
      "84    46656       56134          104.9           233.6           440   \n",
      "85    73066       45713          271.9           187.0           642   \n",
      "86    14642       49441           37.6           297.3           143   \n",
      "87    33854       52346           97.4           134.5           202   \n",
      "88     4190       37736           10.6           180.4            12   \n",
      "89   219588       64885          489.1           211.6           901   \n",
      "90    45193       41971          101.9           605.9           322   \n",
      "91  1006740       75155         2746.1           246.8          2935   \n",
      "92    20470       42947           51.1           198.9            56   \n",
      "93    12509       43297           29.0           502.6            26   \n",
      "94    54335       58893          117.3           107.1           147   \n",
      "95   125055       49444          469.0           392.7           669   \n",
      "96    69581       41762          240.6           215.7           635   \n",
      "97    81336       46399          310.9           411.9           310   \n",
      "98    38011       56134          165.4           297.0           365   \n",
      "99    18061       44548           73.3            67.6           101   \n",
      "\n",
      "    case_disp_ratio  probability  prediction  \n",
      "80            1.380     0.700513           1  \n",
      "81            1.681     0.468595           0  \n",
      "82            1.613     0.548461           1  \n",
      "83            1.449     0.513243           1  \n",
      "84            2.008     0.492042           0  \n",
      "85            1.009     0.524625           1  \n",
      "86            1.040     0.533705           1  \n",
      "87            1.136     0.659597           1  \n",
      "88            1.396     0.747475           1  \n",
      "89            1.723     0.191916           0  \n",
      "90            1.103     0.508861           1  \n",
      "91            1.727     0.010227           0  \n",
      "92            1.150     0.612590           1  \n",
      "93            0.930     0.621484           1  \n",
      "94            1.706     0.478729           0  \n",
      "95            2.111     0.550491           1  \n",
      "96            2.815     0.553422           1  \n",
      "97            2.053     0.481768           0  \n",
      "98            2.123     0.498155           0  \n",
      "99            1.861     0.660995           1   \n",
      "\n",
      "Test Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# use log model to predict the targets and model test accuracy\n",
    "x_test = county_predict.run_and_predict_results().X_test\n",
    "# print a few rows of the predicted ouptuts dataframe\n",
    "print(county_predict.run_and_predict_results().df_preprocessed.tail(20), '\\n')\n",
    "print(\"Test Score:\", county_predict.run_and_predict_results().log_model_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Predictions and Summary Table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PredictCaseDispIneff at 0x17185fdb3c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use method to initialize connection to MySQL, connect to database with login credentials, inserts all rows of df_preprocessed, and saves database\n",
    "county_predict.insert_into_mysql()\n",
    "county_predict.st_into_mysql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Stored Database and Tables in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dicitionary to use **kwargs for the connection\n",
    "sql_creds = {'database':'predictions', 'user':'root', 'password':'pass_word'}\n",
    "# establish connection using pymysql module\n",
    "conn = pymysql.connect(**sql_creds)\n",
    "# create cursor for connection\n",
    "cursor = conn.cursor()\n",
    "# create queries to print all rows of predicted outputs table and summary table in MySQL\n",
    "pred_query = \"SELECT * FROM predicted_outputs;\"\n",
    "# lists summary table in order of greatest to least odds ratios for the features\n",
    "st_query = \"SELECT * FROM summary_table ORDER BY odds_ratio DESC;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alamance', 15, 34, 8, 0, 7, 158137, 51303, 510.7, 422.2, 773, 1.951, 0.332811, 0)\n",
      "('Alexander', 4, 10, 0, 0, 2, 37737, 49864, 88.4, 176.8, 225, 1.436, 0.516376, 1)\n",
      "('Alleghany', 3, 6, 2, 0, 3, 11224, 40420, 48.3, 185.8, 53, 1.744, 0.615792, 1)\n",
      "('Anson', 4, 8, 2, 0, 4, 25949, 39278, 68.7, 511.0, 87, 1.95, 0.518445, 1)\n",
      "('Ashe', 2, 7, 0, 0, 6, 27212, 46667, 93.9, 108.2, 135, 1.377, 0.615559, 1)\n",
      "('Avery', 3, 5, 2, 0, 7, 17881, 47258, 47.0, 126.9, 91, 2.021, 0.65256, 1)\n",
      "('Beaufort', 6, 16, 7, 1, 9, 47580, 50307, 182.9, 307.2, 295, 1.159, 0.656239, 1)\n",
      "('Bertie', 2, 7, 0, 0, 5, 20169, 40428, 44.7, 159.8, 57, 0.786, 0.639162, 1)\n",
      "('Bladen', 6, 12, 2, 0, 7, 34833, 42097, 80.7, 225.2, 183, 1.399, 0.647359, 1)\n",
      "('Brunswick', 6, 31, 10, 0, 14, 123657, 58224, 297.1, 156.7, 718, 1.591, 0.449665, 0)\n",
      "\n",
      "      county  dist_judg_mag  clerks  dist_attys  p_def  law_enf_ag  avg_pop  \\\n",
      "0   Alamance             15      34           8      0           7   158137   \n",
      "1  Alexander              4      10           0      0           2    37737   \n",
      "2  Alleghany              3       6           2      0           3    11224   \n",
      "3      Anson              4       8           2      0           4    25949   \n",
      "4       Ashe              2       7           0      0           6    27212   \n",
      "5      Avery              3       5           2      0           7    17881   \n",
      "6   Beaufort              6      16           7      1           9    47580   \n",
      "7     Bertie              2       7           0      0           5    20169   \n",
      "8     Bladen              6      12           2      0           7    34833   \n",
      "9  Brunswick              6      31          10      0          14   123657   \n",
      "\n",
      "   avg_income  avg_num_divrs  vcr_per_100000  op_od_deaths  case_disp_ratio  \\\n",
      "0       51303          510.7           422.2           773            1.951   \n",
      "1       49864           88.4           176.8           225            1.436   \n",
      "2       40420           48.3           185.8            53            1.744   \n",
      "3       39278           68.7           511.0            87            1.950   \n",
      "4       46667           93.9           108.2           135            1.377   \n",
      "5       47258           47.0           126.9            91            2.021   \n",
      "6       50307          182.9           307.2           295            1.159   \n",
      "7       40428           44.7           159.8            57            0.786   \n",
      "8       42097           80.7           225.2           183            1.399   \n",
      "9       58224          297.1           156.7           718            1.591   \n",
      "\n",
      "   probability  prediction  \n",
      "0     0.332811           0  \n",
      "1     0.516376           1  \n",
      "2     0.615792           1  \n",
      "3     0.518445           1  \n",
      "4     0.615559           1  \n",
      "5     0.652560           1  \n",
      "6     0.656239           1  \n",
      "7     0.639162           1  \n",
      "8     0.647359           1  \n",
      "9     0.449665           0  \n"
     ]
    }
   ],
   "source": [
    "# execute predictions query\n",
    "cursor.execute(pred_query)\n",
    "# assign variable to all rows\n",
    "predictions = cursor.fetchall()\n",
    "# print a few rows of the predictions table\n",
    "for row in predictions[:10]:\n",
    "    print(row)\n",
    "\n",
    "# prints new line\n",
    "print()\n",
    "# compare this to df_preprocessed in model class\n",
    "predictions_df = county_predict.df_preprocessed\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('p_def', 0.566896, 1.76279)\n",
      "('dist_judg_mag', 0.465631, 1.59302)\n",
      "('op_od_deaths', 0.345726, 1.41302)\n",
      "('law_enf_ag', 0.334558, 1.39732)\n",
      "('dist_attys', -0.13954, 0.869758)\n",
      "('vcr_per_100000', -0.222799, 0.800275)\n",
      "('avg_income', -0.28207, 0.754221)\n",
      "('avg_pop', -0.481183, 0.618052)\n",
      "('clerks', -0.6373, 0.528718)\n",
      "('avg_num_divrs', -0.671237, 0.511076)\n",
      "\n",
      "     feature_name  coefficient  odds_ratio\n",
      "3           p_def     0.566896    1.762788\n",
      "0   dist_judg_mag     0.465631    1.593020\n",
      "9    op_od_deaths     0.345726    1.413015\n",
      "4      law_enf_ag     0.334558    1.397323\n",
      "2      dist_attys    -0.139540    0.869758\n",
      "8  vcr_per_100000    -0.222799    0.800275\n",
      "6      avg_income    -0.282070    0.754221\n",
      "5         avg_pop    -0.481183    0.618052\n",
      "1          clerks    -0.637300    0.528718\n",
      "7   avg_num_divrs    -0.671237    0.511076\n"
     ]
    }
   ],
   "source": [
    "# execute summary_table query\n",
    "cursor.execute(st_query)\n",
    "# assign variable to all rows\n",
    "summary_table = cursor.fetchall()\n",
    "# print a few rows of the summary_table database table\n",
    "for row in summary_table:\n",
    "    print(row)\n",
    "    \n",
    "# prints new line\n",
    "print()\n",
    "# compare this to summary_table within the model class\n",
    "summary_table_df = county_predict.summary_table\n",
    "print(summary_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Highlights Based on Model with Tableau Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type='text/javascript' src='https://prod-useast-a.online.tableau.com/javascripts/api/viz_v1.js'></script><div class='tableauPlaceholder' style='width: 1000px; height: 827px;'><object class='tableauViz' width='1000' height='827' style='display:none;'><param name='host_url' value='https%3A%2F%2Fprod-useast-a.online.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='&#47;t&#47;dataprojectsdontreavery' /><param name='name' value='TargetCountiesInefficiencyvsEfficiency&#47;TargetCountiesStats' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='showAppBanner' value='false' /></object></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script type='text/javascript' src='https://prod-useast-a.online.tableau.com/javascripts/api/viz_v1.js'></script><div class='tableauPlaceholder' style='width: 1000px; height: 827px;'><object class='tableauViz' width='1000' height='827' style='display:none;'><param name='host_url' value='https%3A%2F%2Fprod-useast-a.online.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='&#47;t&#47;dataprojectsdontreavery' /><param name='name' value='TargetCountiesInefficiencyvsEfficiency&#47;TargetCountiesStats' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='showAppBanner' value='false' /></object></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Disposition Ratio by Geographic Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type='text/javascript' src='https://prod-useast-a.online.tableau.com/javascripts/api/viz_v1.js'></script><div class='tableauPlaceholder' style='width: 1536px; height: 669px;'><object class='tableauViz' width='1536' height='669' style='display:none;'><param name='host_url' value='https%3A%2F%2Fprod-useast-a.online.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='&#47;t&#47;dataprojectsdontreavery' /><param name='name' value='TargetCountiesInefficiencyvsEfficiency&#47;CaseDispByCounty' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='showAppBanner' value='false' /></object></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script type='text/javascript' src='https://prod-useast-a.online.tableau.com/javascripts/api/viz_v1.js'></script><div class='tableauPlaceholder' style='width: 1536px; height: 669px;'><object class='tableauViz' width='1536' height='669' style='display:none;'><param name='host_url' value='https%3A%2F%2Fprod-useast-a.online.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='&#47;t&#47;dataprojectsdontreavery' /><param name='name' value='TargetCountiesInefficiencyvsEfficiency&#47;CaseDispByCounty' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='showAppBanner' value='false' /></object></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "The model predicted 58 counties to be inefficient when disposing of felony cases, which accounts for approximately 60% of all counties in North Carolina.  Tyrrell, Northampton, and Polk are among the top three counties needing additional resources, according to the probability of the model. The accuracy of the model's test score is 16% less than the training score.  This may have occurred as a result of model overfitting with regards to a random_state=341, used ONLY for obtaining highest model accuracy.  However, the model detected several features that could be determining factors of case disposition efficiency within a county.  Whether a county would have at least one public defender has the greatest weight in the summary table.  Anyone charged with a crime, felony in this case, who is at risk of jail confinement and cannot afford an attorney is appointed a public defender.  Counties without public defender offices are likely to face adversity in disposing felony cases, especially poverty-stricken areas.  The number of district court judges and magistrates rank second in weight of the model.  Counties well equipped with a sufficient number of district court judges and magistrates are likely more prepared to dispose district cases indicted to superior court.  The number of opioid overdose deaths is the next most important feature and could suggest that the astronomical increase in opioid abuse calls for increased felony criminal activity resulting in more filed cases.  Lastly, the number of law enforcement agencies carries significant weight in the model.  A reasonable explanation is the theory that a shortage in law enforcement agencies within a county implicates backlog for pending cases.  An increase in caseload for law enforcement causes stagnancy in personnel who interact with NC's case maintenance.  One final note on this project is the possibility of an existing relationship between county location and case disposition ratio.  The map visualization above shows a group of counties in the upper right of the Coastal Plains with relatively low case disposition ratios such as Bertie, Washington, and Hertford counties.  This may be a reason to investigate this observation further for reasons possibly not mentioned in this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
